# Configuration Overview

**India AI Policy Tracker - Service Dependencies & Configuration**

Last Updated: January 27, 2026

---

## ğŸ“‹ Table of Contents

1. [System Overview](#system-overview)
2. [External Services](#external-services)
3. [API Keys Configuration](#api-keys-configuration)
4. [Provider Switching](#provider-switching)
5. [Configuration Files](#configuration-files)
6. [Troubleshooting](#troubleshooting)

---

## ğŸ—ï¸ System Overview

The India AI Tracker uses a **3-layer hybrid processing pipeline** to minimize costs while maintaining quality:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 1: Rule-Based Filter (FREE)              â”‚
â”‚ - No external services                          â”‚
â”‚ - Pure Python keyword matching                  â”‚
â”‚ - 2,000-3,500 articles â†’ ~1,200-1,400 articles â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 2: Bulk AI Processing                    â”‚
â”‚ - Provider: Groq (primary) OR Ollama (fallback)â”‚
â”‚ - 1,200-1,400 articles â†’ ~900-1,100 articles   â”‚
â”‚ - Cost: $0 (Groq free tier)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 3: Premium Polish                        â”‚
â”‚ - Provider: Gemini 1.5 Flash (primary)         â”‚
â”‚ - Top 30-50 important articles only            â”‚
â”‚ - Cost: $0 (within free tier)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸŒ External Services

### **Services We Use:**

| Service | Purpose | Required? | Fallback |
|---------|---------|-----------|----------|
| **Groq API** | Layer 2 bulk processing | Yes | Ollama (local) |
| **Gemini API** | Layer 3 premium polish | Yes | Groq |
| **Ollama** | Layer 2 fallback (local) | Optional | None |

### **Services We DON'T Use:**

- âŒ No database hosting (SQLite runs locally)
- âŒ No server hosting for processing (runs on your Mac)
- âŒ No paid AI APIs (all within free tiers)

---

## ğŸ”‘ API Keys Configuration

### **1. Groq API Key**

**Purpose:** Primary provider for Layer 2 bulk processing

**Free Tier:**
- 14,400 requests/day
- 131,072 tokens/minute
- Model: Llama 3.1 70B

**How to Get:**
1. Go to https://console.groq.com/
2. Sign up with email
3. Navigate to "API Keys"
4. Create new key
5. Copy the key (starts with `gsk_...`)

**Configuration:**
```bash
# In backend/.env file
GROQ_API_KEY=gsk_your_key_here
```

---

### **2. Gemini API Key**

**Purpose:** Premium provider for Layer 3 polish

**Free Tier:**
- 1,500 requests/day (we use ~30-50)
- 1M tokens/minute
- Model: Gemini 1.5 Flash

**How to Get:**
1. Go to https://ai.google.dev/
2. Click "Get API Key"
3. Create project in Google Cloud
4. Enable Generative Language API
5. Copy key (starts with `AIza...`)

**Configuration:**
```bash
# In backend/.env file
GEMINI_API_KEY=AIzaSyD...your_key_here
```

**Current Key (in your .env):**
```
GEMINI_API_KEY=AIzaSyDG-kN6DPIYKYBdf9d0wnjNxEA0XDF7gKc
```

---

### **3. Ollama (Optional Local Fallback)**

**Purpose:** Fallback when Groq hits rate limits

**Free:** 100% free, runs locally on your Mac

**Installation:**
```bash
# Install Ollama
brew install ollama

# Start Ollama service
ollama serve

# Pull Llama 3.2 3B model (~2GB)
ollama pull llama3.2:3b
```

**Configuration:**
```bash
# In backend/.env file (optional)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
```

**No API key needed** - runs entirely offline

---

## ğŸ”„ Provider Switching

### **Layer 2: Groq â†’ Ollama Switching**

**Automatic Fallback:**
The system automatically switches to Ollama if Groq fails:

```python
# Happens automatically when:
# - Groq rate limit exceeded
# - Groq API timeout
# - Groq quota exhausted
# - Network error

# You'll see in reports:
# âš ï¸  Layer 2 switched from Groq to Ollama (123 articles remaining)
```

**Manual Configuration:**
```bash
# In backend/.env file

# Use Groq (default)
LAYER2_PROVIDER=groq

# Force Ollama
LAYER2_PROVIDER=ollama

# Use Gemini for Layer 2 (not recommended - slower/expensive)
LAYER2_PROVIDER=gemini
```

---

### **Layer 3: Gemini â†” Groq Switching**

**Manual Configuration:**
```bash
# In backend/.env file

# Use Gemini (default - recommended)
LAYER3_PROVIDER=gemini

# Use Groq instead
LAYER3_PROVIDER=groq
```

**Why Gemini is recommended for Layer 3:**
- Better at nuanced summaries
- Higher quality output
- Still free (we use <50 calls/day)

---

## ğŸ“ Configuration Files

### **1. Environment Variables (.env)**

Location: `backend/.env`

```bash
# ==================== API KEYS ====================
GROQ_API_KEY=gsk_your_key_here
GEMINI_API_KEY=AIzaSyD_your_key_here

# ==================== LAYER 2 CONFIG ====================
LAYER2_PROVIDER=groq           # groq, ollama, or gemini
LAYER2_MODEL=llama-3.1-70b-versatile  # For Groq
LAYER2_BATCH_SIZE=10           # Articles per batch

# ==================== LAYER 3 CONFIG ====================
LAYER3_PROVIDER=gemini         # gemini or groq
LAYER3_MODEL=gemini-1.5-flash  # For Gemini
LAYER3_TOP_N=50                # Number of articles for premium processing

# ==================== OLLAMA CONFIG (Optional) ====================
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# ==================== PROCESSING OPTIONS ====================
ENABLE_CHECKPOINTING=true      # Resume interrupted jobs
CHECKPOINT_INTERVAL=50         # Save progress every N articles
ENABLE_REPORTS=true            # Generate daily reports

# ==================== ADMIN CREDENTIALS ====================
ADMIN_USERNAME=admin
ADMIN_PASSWORD=sreenath
SECRET_KEY=dev-key-change-in-production
```

---

### **2. Filter Configuration (filters.yaml)**

Location: `backend/config/filters.yaml`

**Purpose:** Contains all keywords, weights, and importance hints

**Structure:**
```yaml
ai_keywords:
  strong:  # Must have for AI relevance
    - keyword: "artificial intelligence"
      weight: 100
      categories: ["ai_tech"]
      importance_boost: 0

    - keyword: "AI regulation"
      weight: 150
      categories: ["policy", "government"]
      importance_boost: 30  # Used in Layer 3 scoring

india_markers:
  tier1:  # 50 points - Strong India signals
    states: [...]
    cities: [...]

  tier2:  # 40 points - Indian companies
    companies: [...]

  tier3:  # 60 points - Government/institutions
    government: [...]
    importance_boost: 40

thresholds:
  pass_filter: 40        # Minimum score to pass Layer 1
  high_confidence: 80    # High confidence threshold
  borderline_min: 30     # Log borderline articles
```

**You can edit this file without coding!**

---

## ğŸ”§ Troubleshooting

### **Common Issues:**

#### **1. "Groq rate limit exceeded"**
```
âœ… AUTOMATIC: System switches to Ollama
âŒ If Ollama not installed: Install with `brew install ollama`
```

#### **2. "Ollama connection refused"**
```bash
# Start Ollama service
ollama serve

# In another terminal, pull model
ollama pull llama3.2:3b
```

#### **3. "Gemini quota exceeded"**
```
âœ… NORMAL: Gemini 2.5 Flash has only 20/day (we don't use it)
âœ… We use Gemini 1.5 Flash (1,500/day)
ğŸ”§ Fix: Update .env to use gemini-1.5-flash model
```

#### **4. "API key invalid"**
```bash
# Test Groq key
curl -H "Authorization: Bearer $GROQ_API_KEY" \
     https://api.groq.com/openai/v1/models

# Test Gemini key
curl "https://generativelanguage.googleapis.com/v1beta/models?key=$GEMINI_API_KEY"
```

---

## ğŸ“Š Cost Monitoring

### **Check Your Usage:**

**Groq:**
- Dashboard: https://console.groq.com/usage
- Free tier: 14,400 requests/day
- We use: ~1,200/day (8%)

**Gemini:**
- Dashboard: https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/quotas
- Free tier: 1,500 requests/day
- We use: ~30-50/day (3%)

### **Daily Reports Show Usage:**
```
LAYER 2: Bulk Processing
  Provider:   Groq
  âš ï¸  Groq quota used: 1,247/14,400 (8.7%)

LAYER 3: Premium Polish
  Provider:   Gemini 1.5 Flash
  Gemini quota used: 42/1,500 (2.8%)
```

---

## ğŸ†˜ Getting Help

**If something goes wrong:**

1. Check `reports/daily_report_YYYY-MM-DD.json` for errors
2. Check `backend/logs/processor.log` for detailed logs
3. Check this file for configuration guidance
4. Restore from backup: `git checkout backup-before-hybrid-pipeline`

**Configuration Questions:**
- Which provider should I use? â†’ Use defaults (Groq + Gemini)
- Can I use only free services? â†’ Yes! Everything is free
- Do I need Ollama? â†’ Optional, but recommended as fallback

---

## ğŸ“ Summary

**Required for Operation:**
- âœ… Groq API key (free)
- âœ… Gemini API key (free)
- âš ï¸  Ollama (optional but recommended)

**Monthly Cost:**
- âœ… $0 if within free tiers (normal usage)
- âš ï¸  <$1 if you exceed free tiers (unlikely)

**Next Steps:**
- Read `docs/ARCHITECTURE.md` for system design
- Read `scripts/README.md` for daily operations
