# System Architecture

**India AI Policy Tracker - 3-Layer Hybrid Processing Pipeline**

Last Updated: January 27, 2026

---

## ğŸ¯ Design Goals

1. **Minimize Cost**: Target $0/month (within free tiers)
2. **Maintain Quality**: Keep sophisticated filtering logic
3. **Mac-Friendly**: No heavy GPU requirements
4. **Resilient**: Automatic fallbacks if services fail
5. **Transparent**: Clear reporting at each stage

---

## ğŸ“Š High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DAILY SCRAPING                            â”‚
â”‚  220 sources â†’ 2,000-3,500 new articles                      â”‚
â”‚  (RSS feeds + web scrapers)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 1: RULE-BASED FILTER                                   â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚ Module: ai/rule_filter.py                                     â”‚
â”‚ Config: config/filters.yaml                                   â”‚
â”‚                                                               â”‚
â”‚ âœ“ 125+ AI keywords (from existing filter.py)                â”‚
â”‚ âœ“ 5-tier India relevance scoring (50/40/30/20/10 pts)       â”‚
â”‚ âœ“ False positive prevention                                  â”‚
â”‚ âœ“ Confidence zones: HIGH / MEDIUM / BORDERLINE              â”‚
â”‚                                                               â”‚
â”‚ Cost:  $0 (pure Python)                                      â”‚
â”‚ Time:  <1 second                                             â”‚
â”‚ Pass Rate: ~40-45% (1,200-1,400 articles)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 2: BULK AI PROCESSING                                  â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚ Module: ai/layer2_processor.py                               â”‚
â”‚ Provider: Groq (primary) â†’ Ollama (fallback)                â”‚
â”‚                                                               â”‚
â”‚ Processes in batches of 10 articles:                         â”‚
â”‚ âœ“ AI relevance check (reuses filter.py logic)               â”‚
â”‚ âœ“ Category assignment (reuses categoriser.py)               â”‚
â”‚ âœ“ State attribution (reuses geo_attributor.py)              â”‚
â”‚ âœ“ Basic summary (reuses summarizer.py)                      â”‚
â”‚                                                               â”‚
â”‚ Checkpointing: Saves progress every 50 articles              â”‚
â”‚ Auto-fallback: Groq â†’ Ollama on rate limit                  â”‚
â”‚                                                               â”‚
â”‚ Cost:  $0 (Groq free tier: 14,400/day)                      â”‚
â”‚ Time:  15-20 minutes (Groq) or 2-3 hours (Ollama)           â”‚
â”‚ Pass Rate: ~70-80% AI-relevant (900-1,100 articles)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 3: PREMIUM POLISH                                      â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚ Module: ai/importance_scorer.py + ai/premium_client.py       â”‚
â”‚ Provider: Gemini 1.5 Flash                                   â”‚
â”‚                                                               â”‚
â”‚ Importance Scoring:                                          â”‚
â”‚ âœ“ Union/Central government mentions (+30 pts)               â”‚
â”‚ âœ“ Ministry/Parliament/PM (+25 pts)                          â”‚
â”‚ âœ“ Funding > â‚¹10 crore (+20 pts)                            â”‚
â”‚ âœ“ Policy keywords from YAML (+15 pts)                       â”‚
â”‚ âœ“ National scope (+10 pts)                                  â”‚
â”‚ âœ“ Major institutions (+10 pts)                              â”‚
â”‚                                                               â”‚
â”‚ Top 30-50 articles get:                                      â”‚
â”‚ âœ“ Refined categorization                                    â”‚
â”‚ âœ“ Cross-checked state attribution                           â”‚
â”‚ âœ“ Polished 2-3 line summary                                 â”‚
â”‚                                                               â”‚
â”‚ Cost:  $0 (Gemini free tier: 1,500/day, use ~40)           â”‚
â”‚ Time:  1-2 minutes                                           â”‚
â”‚ Output: 30-50 premium articles                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FINAL OUTPUT                                â”‚
â”‚  900-1,100 processed articles ready for website              â”‚
â”‚  (30-50 with premium polish)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—‚ï¸ Module Relationships

### **Existing Modules (Preserved)**

These are your painstakingly built modules that contain all the sophisticated logic:

```
ai/filter.py (28KB)
â”œâ”€â”€ STRONG_AI_KEYWORDS [125+ patterns]
â”œâ”€â”€ CONTEXT_DEPENDENT_KEYWORDS
â”œâ”€â”€ TIER2_INDIAN_COMPANIES [100+ companies]
â”œâ”€â”€ TIER2_GLOBAL_INDIA
â”œâ”€â”€ TIER3_INSTITUTIONS
â”œâ”€â”€ TIER4_CURRENCY
â”œâ”€â”€ check_relevance(title, content)
â””â”€â”€ Used by: Layer 1 (extraction) + Layer 2 (AI call)

ai/deduplicator.py (24KB)
â”œâ”€â”€ DEDUP_WINDOW_DAYS = 14
â”œâ”€â”€ Fuzzy title matching
â”œâ”€â”€ URL canonicalization
â”œâ”€â”€ Entity extraction
â””â”€â”€ Used by: Scraper (before Layer 1)

ai/geo_attributor.py (50KB)
â”œâ”€â”€ State pattern matching
â”œâ”€â”€ City-to-state mapping
â”œâ”€â”€ Company HQ detection
â””â”€â”€ Used by: Layer 2 + Layer 3

ai/categoriser.py (27KB)
â”œâ”€â”€ Category assignment logic
â”œâ”€â”€ Policy vs tech vs startup classification
â””â”€â”€ Used by: Layer 2 + Layer 3

ai/summarizer.py (3KB)
â”œâ”€â”€ Summary generation prompts
â””â”€â”€ Used by: Layer 2 + Layer 3

ai/date_extractor.py (7KB)
â”œâ”€â”€ Date parsing from content
â””â”€â”€ Used by: Scraper
```

### **New Modules (To Be Created)**

These are thin wrappers and orchestrators that use your existing modules:

```
ai/rule_filter.py (NEW)
â”œâ”€â”€ Extracts keywords from filter.py into fast lookup
â”œâ”€â”€ Implements scoring without AI calls
â”œâ”€â”€ Confidence zones: HIGH/MEDIUM/BORDERLINE
â””â”€â”€ Purpose: Fast pre-filter (Layer 1)

ai/layer2_processor.py (NEW)
â”œâ”€â”€ Orchestrates filter.py + categoriser.py + geo_attributor.py + summarizer.py
â”œâ”€â”€ Batches 10 articles per API call
â”œâ”€â”€ Handles checkpointing and resume
â”œâ”€â”€ Automatic Groq â†’ Ollama fallback
â””â”€â”€ Purpose: Efficient bulk processing (Layer 2)

ai/importance_scorer.py (NEW)
â”œâ”€â”€ Reuses importance_boost hints from filters.yaml
â”œâ”€â”€ Content analysis (govt mentions, funding, etc.)
â”œâ”€â”€ Manual overrides (force_premium flag)
â””â”€â”€ Purpose: Select top articles (Layer 3 input)

ai/premium_client.py (NEW)
â”œâ”€â”€ Unified interface for Gemini/Groq
â”œâ”€â”€ Calls filter + categoriser + geo + summarizer again with premium model
â”œâ”€â”€ Cross-checks and refines Layer 2 results
â””â”€â”€ Purpose: Polish top articles (Layer 3)

ai/providers/groq_client.py (NEW)
â”œâ”€â”€ Groq API wrapper
â”œâ”€â”€ Batch processing support
â””â”€â”€ Rate limit handling

ai/providers/ollama_client.py (NEW)
â”œâ”€â”€ Ollama local model wrapper
â”œâ”€â”€ Same interface as groq_client
â””â”€â”€ Fallback option

ai/providers/gemini_client.py (UPDATE EXISTING)
â”œâ”€â”€ Already exists (gemini_api.py)
â”œâ”€â”€ Update for Layer 3 batch processing
â””â”€â”€ Provider for premium polish

ai/checkpoint_manager.py (NEW)
â”œâ”€â”€ Save/restore processing state
â”œâ”€â”€ Resume interrupted jobs
â””â”€â”€ Idempotent operations

ai/report_generator.py (NEW)
â”œâ”€â”€ Console output with colors
â”œâ”€â”€ JSON report generation
â”œâ”€â”€ Provider usage tracking
â””â”€â”€ Fallback event logging
```

---

## ğŸ”„ Data Flow

### **1. Scraping Phase**

```
run_scraper_only.py
â”‚
â”œâ”€> Load sources from sources.json (220 sources)
â”‚
â”œâ”€> For each source:
â”‚   â”œâ”€> RSS scraper OR web scraper
â”‚   â”œâ”€> Extract: title, url, content, date, source
â”‚   â””â”€> Output: Raw article data
â”‚
â”œâ”€> Deduplication (ai/deduplicator.py)
â”‚   â”œâ”€> Check against last 14 days
â”‚   â”œâ”€> Fuzzy title matching
â”‚   â”œâ”€> Entity extraction
â”‚   â””â”€> Output: 2,000-3,500 unique articles
â”‚
â””â”€> Save to database with processing_state='SCRAPED'
```

### **2. Layer 1: Rule Filter**

```
ai/rule_filter.py
â”‚
â”œâ”€> Load config/filters.yaml
â”‚   â”œâ”€> 125+ AI keywords with weights
â”‚   â””â”€> India markers (states, companies, govt)
â”‚
â”œâ”€> For each SCRAPED article:
â”‚   â”‚
â”‚   â”œâ”€> Score AI relevance (0-150 pts)
â”‚   â”‚   â”œâ”€> Check title for keywords
â”‚   â”‚   â”œâ”€> Check first 500 chars of content
â”‚   â”‚   â””â”€> Weight: strong (100) > medium (50) > policy (150)
â”‚   â”‚
â”‚   â”œâ”€> Score India relevance (0-60 pts)
â”‚   â”‚   â”œâ”€> Tier 1: States in title (50 pts)
â”‚   â”‚   â”œâ”€> Tier 2: Indian companies (40 pts)
â”‚   â”‚   â”œâ”€> Tier 3: Govt/institutions (60 pts)
â”‚   â”‚   â””â”€> Tier 4: Currency mentions (20 pts)
â”‚   â”‚
â”‚   â”œâ”€> Total score = AI score + India score
â”‚   â”‚
â”‚   â””â”€> Decision:
â”‚       â”œâ”€> Score >= 80: HIGH confidence â†’ Pass to Layer 2
â”‚       â”œâ”€> Score 40-79: MEDIUM confidence â†’ Pass to Layer 2
â”‚       â”œâ”€> Score 30-39: BORDERLINE â†’ Log for review + Pass to Layer 2
â”‚       â””â”€> Score < 30: REJECT â†’ Mark as not relevant
â”‚
â””â”€> Output: 1,200-1,400 articles with rule_filter_score + confidence
```

### **3. Layer 2: Bulk Processing**

```
ai/layer2_processor.py
â”‚
â”œâ”€> Load SCRAPED articles that passed Layer 1
â”‚
â”œâ”€> Check for checkpoint (resume if interrupted)
â”‚
â”œâ”€> Process in batches of 10:
â”‚   â”‚
â”‚   â”œâ”€> Try Groq API:
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€> Build combined prompt:
â”‚   â”‚   â”‚   â”œâ”€> Article 1: [title + content]
â”‚   â”‚   â”‚   â”œâ”€> Article 2: [title + content]
â”‚   â”‚   â”‚   â”œâ”€> ...
â”‚   â”‚   â”‚   â”œâ”€> Article 10: [title + content]
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€> "For EACH article, provide:
â”‚   â”‚   â”‚       1. AI relevance (YES/NO + score)
â”‚   â”‚   â”‚       2. Category
â”‚   â”‚   â”‚       3. State codes (JSON array)
â”‚   â”‚   â”‚       4. Summary (2-3 sentences)"
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€> Parse structured JSON response
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€> Save results to database
â”‚   â”‚
â”‚   â”œâ”€> On RateLimitError:
â”‚   â”‚   â”œâ”€> Log fallback event
â”‚   â”‚   â”œâ”€> Switch to Ollama
â”‚   â”‚   â””â”€> Continue processing
â”‚   â”‚
â”‚   â””â”€> Checkpoint every 50 articles
â”‚
â”œâ”€> Filter out non-AI-relevant articles
â”‚
â””â”€> Output: 900-1,100 AI-relevant articles with Layer 2 processing
```

### **4. Layer 3: Premium Polish**

```
ai/importance_scorer.py
â”‚
â”œâ”€> Load all articles that passed Layer 2
â”‚
â”œâ”€> For each article:
â”‚   â”‚
â”‚   â”œâ”€> Calculate importance score:
â”‚   â”‚   â”œâ”€> Union govt mentions: +30
â”‚   â”‚   â”œâ”€> Ministry/Parliament/PM: +25
â”‚   â”‚   â”œâ”€> Funding > â‚¹10 crore: +20
â”‚   â”‚   â”œâ”€> Policy keywords (from YAML): +15
â”‚   â”‚   â”œâ”€> National scope: +10
â”‚   â”‚   â”œâ”€> Major institutions: +10
â”‚   â”‚   â””â”€> importance_boost from YAML metadata
â”‚   â”‚
â”‚   â”œâ”€> Check manual overrides:
â”‚   â”‚   â”œâ”€> force_premium flag: score = 999
â”‚   â”‚   â””â”€> skip_premium flag: score = -999
â”‚   â”‚
â”‚   â””â”€> Save importance_score to database
â”‚
â”œâ”€> Sort by importance_score DESC
â”‚
â”œâ”€> Select top 30-50 articles
â”‚
â””â”€> Pass to Premium Client
    â”‚
    ai/premium_client.py
    â”‚
    â”œâ”€> For each top article:
    â”‚   â”‚
    â”‚   â”œâ”€> Call Gemini 1.5 Flash with refined prompt:
    â”‚   â”‚   â”œâ”€> "This is a high-importance article"
    â”‚   â”‚   â”œâ”€> "Cross-check the category"
    â”‚   â”‚   â”œâ”€> "Verify state attribution"
    â”‚   â”‚   â””â”€> "Produce polished summary"
    â”‚   â”‚
    â”‚   â”œâ”€> Update database with refined results
    â”‚   â”‚
    â”‚   â””â”€> Mark premium_processed = True
    â”‚
    â””â”€> Output: 30-50 premium-polished articles
```

---

## ğŸ’¾ Database Schema

### **Update Model (Enhanced)**

```python
class Update(db.Model):
    # Existing fields (unchanged)
    id = db.Column(db.Integer, primary_key=True)
    title = db.Column(db.String(500), nullable=False)
    url = db.Column(db.String(1000), unique=True)
    content = db.Column(db.Text)
    date_published = db.Column(db.Date)
    date_scraped = db.Column(db.DateTime)
    source_name = db.Column(db.String(200))

    # Layer 1 results (NEW)
    rule_filter_score = db.Column(db.Float)
    rule_filter_confidence = db.Column(db.String(20))  # high, medium, borderline
    matched_categories = db.Column(db.String(500))  # JSON array

    # Layer 2 results (ENHANCED)
    processing_state = db.Column(db.String(20))  # SCRAPED, PROCESSING, PROCESSED, FAILED
    layer2_processed = db.Column(db.Boolean, default=False)
    layer2_provider = db.Column(db.String(20))  # groq, ollama
    layer2_confidence = db.Column(db.Float)

    # AI results (from Layer 2 or existing system)
    is_ai_relevant = db.Column(db.Boolean)
    relevance_score = db.Column(db.Float)
    category = db.Column(db.String(100))
    state_codes = db.Column(db.String(200))  # JSON
    summary = db.Column(db.Text)

    # Layer 3 results (NEW)
    importance_score = db.Column(db.Float)
    premium_processed = db.Column(db.Boolean, default=False)
    premium_provider = db.Column(db.String(20))  # gemini, groq

    # Manual overrides (NEW)
    force_premium = db.Column(db.Boolean, default=False)
    skip_premium = db.Column(db.Boolean, default=False)

    # Admin (existing)
    is_approved = db.Column(db.Boolean, default=False)
    is_deleted = db.Column(db.Boolean, default=False)
    admin_notes = db.Column(db.Text)
```

### **BorderlineArticle Model (NEW)**

```python
class BorderlineArticle(db.Model):
    """Track borderline cases for filter refinement"""
    id = db.Column(db.Integer, primary_key=True)
    article_id = db.Column(db.Integer, db.ForeignKey('updates.id'))
    rule_score = db.Column(db.Float)  # Score from Layer 1
    layer2_decision = db.Column(db.String(20))  # relevant, not_relevant
    false_positive = db.Column(db.Boolean)  # Set after manual review
    false_negative = db.Column(db.Boolean)
    reviewed = db.Column(db.Boolean, default=False)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
```

---

## ğŸ“‚ File Structure

```
india-ai-tracker/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ filter.py                 â† EXISTING: Your sophisticated filter
â”‚   â”‚   â”œâ”€â”€ deduplicator.py          â† EXISTING: 14-day dedup
â”‚   â”‚   â”œâ”€â”€ geo_attributor.py        â† EXISTING: State detection
â”‚   â”‚   â”œâ”€â”€ categoriser.py           â† EXISTING: Categorization
â”‚   â”‚   â”œâ”€â”€ summarizer.py            â† EXISTING: Summaries
â”‚   â”‚   â”œâ”€â”€ date_extractor.py        â† EXISTING: Date parsing
â”‚   â”‚   â”œâ”€â”€ gemini_api.py            â† EXISTING: Gemini integration
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ rule_filter.py           â† NEW: Layer 1
â”‚   â”‚   â”œâ”€â”€ layer2_processor.py      â† NEW: Layer 2 orchestrator
â”‚   â”‚   â”œâ”€â”€ importance_scorer.py     â† NEW: Layer 3 scoring
â”‚   â”‚   â”œâ”€â”€ premium_client.py        â† NEW: Layer 3 client
â”‚   â”‚   â”œâ”€â”€ checkpoint_manager.py    â† NEW: Resume logic
â”‚   â”‚   â”œâ”€â”€ report_generator.py      â† NEW: Daily reports
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ providers/
â”‚   â”‚       â”œâ”€â”€ groq_client.py       â† NEW
â”‚   â”‚       â”œâ”€â”€ ollama_client.py     â† NEW
â”‚   â”‚       â””â”€â”€ gemini_client.py     â† WRAPPER for gemini_api.py
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ filters.yaml             â† NEW: Extracted keywords
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ update.py                â† ENHANCED: Add layer fields
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ daily_update.sh          â† NEW: Master script
â”‚   â”‚   â”œâ”€â”€ 01_scrape.sh             â† NEW
â”‚   â”‚   â”œâ”€â”€ 02_process.sh            â† NEW
â”‚   â”‚   â””â”€â”€ README.md                â† NEW
â”‚   â”‚
â”‚   â”œâ”€â”€ reports/                     â† NEW: Daily reports
â”‚   â”‚   â””â”€â”€ daily_report_{date}.json
â”‚   â”‚
â”‚   â””â”€â”€ checkpoints/                 â† NEW: Resume files
â”‚       â””â”€â”€ layer2_{date}.json
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ CONFIG_OVERVIEW.md           â† THIS FILE
    â”œâ”€â”€ ARCHITECTURE.md              â† YOU ARE HERE
    â””â”€â”€ TROUBLESHOOTING.md           â† To be created
```

---

## ğŸ”„ Error Handling & Fallbacks

### **Layer 2: Groq â†’ Ollama Fallback**

```python
try:
    results = groq_client.process_batch(articles)
except (RateLimitError, TimeoutError, QuotaExceededError) as e:
    logger.warning(f"Groq failed: {e}")
    logger.info("Switching to Ollama fallback...")

    report.add_fallback_event(
        from_provider="groq",
        to_provider="ollama",
        remaining_count=len(articles),
        error=str(e)
    )

    results = ollama_client.process_batch(articles)
```

### **Checkpoint & Resume**

```python
# Save checkpoint every 50 articles
if processed_count % 50 == 0:
    checkpoint_manager.save({
        'last_processed_id': article.id,
        'processed_count': processed_count,
        'total': total_count,
        'provider': current_provider,
        'started_at': start_time
    })

# Resume from checkpoint
if checkpoint_manager.exists(today):
    checkpoint = checkpoint_manager.load(today)
    start_from_id = checkpoint['last_processed_id']
    logger.info(f"Resuming from article {start_from_id}")
```

---

## ğŸ“Š Performance Characteristics

| Metric | Value |
|--------|-------|
| **Daily Input** | 2,000-3,500 articles |
| **Layer 1 Output** | 1,200-1,400 articles (40% pass) |
| **Layer 2 Output** | 900-1,100 articles (75% pass) |
| **Layer 3 Output** | 30-50 premium articles |
| **Total Time** | 20-30 minutes (Groq) or 2-3 hours (Ollama) |
| **Cost** | $0/month (within free tiers) |
| **API Calls** | ~120-140 (Layer 2) + 30-50 (Layer 3) |

---

## ğŸ¯ Design Decisions

### **Why 3 Layers?**

1. **Layer 1 (Rules):** Eliminate obvious non-matches fast (60% reduction)
2. **Layer 2 (Bulk AI):** Process remaining articles efficiently
3. **Layer 3 (Premium):** Polish only the most important articles

**Alternative considered:** All AI processing â†’ Too expensive (~1,200 Ã— 4 calls = 4,800 calls/day)

### **Why Groq for Layer 2?**

- âœ… 14,400 free requests/day (plenty for 120-140 batch calls)
- âœ… Fast (100+ tokens/sec)
- âœ… Llama 3.1 70B quality
- âŒ Ollama is slower (10 tokens/sec = 2-3 hours)

### **Why Gemini for Layer 3?**

- âœ… Better at nuanced summaries
- âœ… 1,500 free requests/day (use <50)
- âœ… Higher quality than Groq for polish
- âš ï¸  Groq also works as alternative

### **Why Batch Processing?**

Old system: 4 calls per article Ã— 1,200 = 4,800 calls
New system: 1 call per 10 articles = 120 calls

**91.7% API call reduction!**

---

## ğŸ” Security Considerations

1. **API Keys:** Stored in `.env` file (gitignored)
2. **Admin Access:** Basic auth (hardcoded for now)
3. **Database:** Local SQLite (no network exposure)
4. **Frontend:** Static files (no backend attack surface)

---

## ğŸ“ˆ Future Enhancements

**Potential improvements (not in current scope):**

1. **Learning System:** Track false positives/negatives to auto-tune thresholds
2. **Multi-language:** Support regional language sources
3. **Real-time Webhooks:** Push updates to website instantly
4. **Advanced Scoring:** ML-based importance scoring
5. **A/B Testing:** Compare different summarization prompts

---

## âœ… Summary

**This architecture achieves:**

âœ… **Cost:** $0/month (within free tiers)
âœ… **Quality:** Preserves your sophisticated filtering logic
âœ… **Speed:** 20-30 minutes total processing
âœ… **Resilience:** Automatic fallbacks
âœ… **Transparency:** Clear reporting at each stage
âœ… **Maintainability:** Config files you can edit without coding

**Next Steps:**
- Read `scripts/README.md` for daily operations
- Read `CONFIG_OVERVIEW.md` for service setup
